<!DOCTYPE html>
<html lang="en">

<head>
    

  <title>How to build a LLM powered chatbot in python? - Subhajit</title>

  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta
    name="keywords"
    content="blog, ghumkature, Subhajit, jekyll"
  />
  <meta name="author" content="ghumkature" />

  <meta name="description" content="Building an Intelligent RAG-Powered Chatbot with Streamlit and LangChain" />
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link
    href="https://fonts.googleapis.com/css2?family=Syne:wght@400..800&display=swap"
    rel="stylesheet"
  />
  <link rel="stylesheet" href="http://localhost:4000//css/main.css" />
  <link rel="icon" type="image/ico" href="http://localhost:4000//assets/favicon.ico" />
  <link rel="shortcut-icon" type="image/ico" href="http://localhost:4000//assets/favicon.ico" />

  <!-- For Facebook -->
  <meta property="og:title" content="How to build a LLM powered chatbot in python? - Subhajit" />
  <meta property="og:image" itemprop="image" content="http://localhost:4000//assets/favicon.ico" />
  <meta property="og:description" content="Building an Intelligent RAG-Powered Chatbot with Streamlit and LangChain" />

  <!-- For Twitter -->
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="How to build a LLM powered chatbot in python? - Subhajit" />
  <meta name="twitter:description" content="Building an Intelligent RAG-Powered Chatbot with Streamlit and LangChain" />
  <meta name="twitter:image" content="http://localhost:4000//assets/favicon.ico" />
</head>


<body>
    <div class="container">
        <div class="navbar">
    <a class="site-title" href="http://localhost:4000//">Subhajit</a>

    <ul class="pull-right">
        
        <li class="pull-left">
            
            <a href="http://localhost:4000//about">About</a>
            /
        </li>
        
        <li class="pull-left">
            
            <a href="https://subhajit-paul.vercel.app/about">Portfolio</a>
            
        </li>
        
    </ul>

    <hr>
</div>
        <div class="page-title">
            How to build a LLM powered chatbot in python?
        </div>
        <div class="content">
            <div class="page-subtitle">
    
        
            <b>[</b>
        
        <a href="//tags/#langchain" title="langchain">langchain</a>
        
            ,
        
    
        
        <a href="//tags/#streamlit" title="streamlit">streamlit</a>
        
            ,
        
    
        
        <a href="//tags/#LLM" title="LLM">LLM</a>
        
            ,
        
    
        
        <a href="//tags/#RAG" title="RAG">RAG</a>
        
            <b>]</b>
        
    
</div>

<h1 id="building-an-intelligent-rag-powered-chatbot-with-streamlit-and-langchain">Building an Intelligent RAG-Powered Chatbot with Streamlit and LangChain</h1>

<p>In today’s rapidly evolving landscape of conversational AI, the integration of Retrieval-Augmented Generation (RAG) with modern web frameworks has emerged as a powerful approach for creating context-aware, knowledge-grounded chatbots. This article explores the implementation of a sophisticated chatbot system that leverages Streamlit for the user interface, LangChain for orchestration, and RAG for enhanced response generation.</p>

<h2 id="understanding-the-core-components">Understanding the Core Components</h2>

<h3 id="retrieval-augmented-generation-rag">Retrieval-Augmented Generation (RAG)</h3>

<p>RAG represents a significant advancement in language model applications, combining the flexibility of generative AI with the accuracy and reliability of retrieval-based systems. Unlike traditional approaches that rely solely on a model’s trained parameters, RAG dynamically incorporates relevant information from a knowledge base during inference, resulting in more accurate and verifiable responses.</p>

<p>The RAG architecture consists of two primary components:</p>
<ol>
  <li>A retriever that identifies and fetches relevant documents from a knowledge base</li>
  <li>A generator that synthesizes these documents with the user query to produce coherent, contextually appropriate responses</li>
</ol>

<h3 id="streamlit-the-frontend-framework">Streamlit: The Frontend Framework</h3>

<p>Streamlit has revolutionized how data scientists and ML engineers build web applications. Its declarative syntax and Python-first approach make it ideal for creating interactive chatbot interfaces. The framework handles state management, user input processing, and real-time updates with minimal boilerplate code.</p>

<h3 id="langchain-the-orchestration-layer">LangChain: The Orchestration Layer</h3>

<p>LangChain serves as the backbone of our chatbot system, providing essential abstractions for:</p>
<ul>
  <li>Document loading and preprocessing</li>
  <li>Vector store management</li>
  <li>Prompt engineering</li>
  <li>Model interaction</li>
  <li>Response generation</li>
</ul>

<h2 id="implementation-architecture">Implementation Architecture</h2>

<h3 id="setting-up-the-development-environment">Setting Up the Development Environment</h3>

<p>First, let’s establish our project environment with the necessary dependencies:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># requirements.txt
</span><span class="n">streamlit</span><span class="o">==</span><span class="mf">1.24</span><span class="p">.</span><span class="mi">0</span>
<span class="n">langchain</span><span class="o">==</span><span class="mf">0.0</span><span class="p">.</span><span class="mi">284</span>
<span class="n">chromadb</span><span class="o">==</span><span class="mf">0.4</span><span class="p">.</span><span class="mi">15</span>
<span class="n">sentence</span><span class="o">-</span><span class="n">transformers</span><span class="o">==</span><span class="mf">2.2</span><span class="p">.</span><span class="mi">2</span>
<span class="n">python</span><span class="o">-</span><span class="n">dotenv</span><span class="o">==</span><span class="mf">1.0</span><span class="p">.</span><span class="mi">0</span>
<span class="n">openai</span><span class="o">==</span><span class="mf">0.28</span><span class="p">.</span><span class="mi">0</span>
</code></pre></div></div>

<h3 id="core-application-structure">Core Application Structure</h3>

<p>Here’s the basic structure of our RAG-powered chatbot:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">streamlit</span> <span class="k">as</span> <span class="n">st</span>
<span class="kn">from</span> <span class="n">langchain.embeddings</span> <span class="kn">import</span> <span class="n">HuggingFaceEmbeddings</span>
<span class="kn">from</span> <span class="n">langchain.vectorstores</span> <span class="kn">import</span> <span class="n">Chroma</span>
<span class="kn">from</span> <span class="n">langchain.chat_models</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>
<span class="kn">from</span> <span class="n">langchain.chains</span> <span class="kn">import</span> <span class="n">ConversationalRetrievalChain</span>
<span class="kn">from</span> <span class="n">langchain.document_loaders</span> <span class="kn">import</span> <span class="n">DirectoryLoader</span>
<span class="kn">from</span> <span class="n">langchain.text_splitter</span> <span class="kn">import</span> <span class="n">RecursiveCharacterTextSplitter</span>

<span class="k">class</span> <span class="nc">RAGChatbot</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="nc">HuggingFaceEmbeddings</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">llm</span> <span class="o">=</span> <span class="nc">ChatOpenAI</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">initialize_knowledge_base</span><span class="p">()</span>
        
    <span class="k">def</span> <span class="nf">initialize_knowledge_base</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="c1"># Load and process documents
</span>        <span class="n">loader</span> <span class="o">=</span> <span class="nc">DirectoryLoader</span><span class="p">(</span><span class="sh">'</span><span class="s">./documents</span><span class="sh">'</span><span class="p">,</span> <span class="n">glob</span><span class="o">=</span><span class="sh">"</span><span class="s">**/*.txt</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">documents</span> <span class="o">=</span> <span class="n">loader</span><span class="p">.</span><span class="nf">load</span><span class="p">()</span>
        
        <span class="c1"># Split documents into chunks
</span>        <span class="n">text_splitter</span> <span class="o">=</span> <span class="nc">RecursiveCharacterTextSplitter</span><span class="p">(</span>
            <span class="n">chunk_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
            <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">200</span>
        <span class="p">)</span>
        <span class="n">splits</span> <span class="o">=</span> <span class="n">text_splitter</span><span class="p">.</span><span class="nf">split_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
        
        <span class="c1"># Create vector store
</span>        <span class="n">self</span><span class="p">.</span><span class="n">vectorstore</span> <span class="o">=</span> <span class="n">Chroma</span><span class="p">.</span><span class="nf">from_documents</span><span class="p">(</span>
            <span class="n">documents</span><span class="o">=</span><span class="n">splits</span><span class="p">,</span>
            <span class="n">embedding</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">embeddings</span>
        <span class="p">)</span>
        
        <span class="c1"># Initialize retrieval chain
</span>        <span class="n">self</span><span class="p">.</span><span class="n">chain</span> <span class="o">=</span> <span class="n">ConversationalRetrievalChain</span><span class="p">.</span><span class="nf">from_llm</span><span class="p">(</span>
            <span class="n">llm</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">llm</span><span class="p">,</span>
            <span class="n">retriever</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">vectorstore</span><span class="p">.</span><span class="nf">as_retriever</span><span class="p">(),</span>
            <span class="n">return_source_documents</span><span class="o">=</span><span class="bp">True</span>
        <span class="p">)</span>
</code></pre></div></div>

<h3 id="streamlit-interface-implementation">Streamlit Interface Implementation</h3>

<p>The user interface is implemented using Streamlit’s components:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">create_ui</span><span class="p">():</span>
    <span class="n">st</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">RAG-Powered Knowledge Assistant</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="c1"># Initialize session state
</span>    <span class="k">if</span> <span class="sh">"</span><span class="s">messages</span><span class="sh">"</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">st</span><span class="p">.</span><span class="n">session_state</span><span class="p">:</span>
        <span class="n">st</span><span class="p">.</span><span class="n">session_state</span><span class="p">.</span><span class="n">messages</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">st</span><span class="p">.</span><span class="n">session_state</span><span class="p">.</span><span class="n">chat_history</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="c1"># Display chat history
</span>    <span class="k">for</span> <span class="n">message</span> <span class="ow">in</span> <span class="n">st</span><span class="p">.</span><span class="n">session_state</span><span class="p">.</span><span class="n">messages</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">st</span><span class="p">.</span><span class="nf">chat_message</span><span class="p">(</span><span class="n">message</span><span class="p">[</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">]):</span>
            <span class="n">st</span><span class="p">.</span><span class="nf">markdown</span><span class="p">(</span><span class="n">message</span><span class="p">[</span><span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">])</span>
    
    <span class="c1"># Chat input
</span>    <span class="k">if</span> <span class="n">prompt</span> <span class="p">:</span><span class="o">=</span> <span class="n">st</span><span class="p">.</span><span class="nf">chat_input</span><span class="p">(</span><span class="sh">"</span><span class="s">What would you like to know?</span><span class="sh">"</span><span class="p">):</span>
        <span class="n">st</span><span class="p">.</span><span class="n">session_state</span><span class="p">.</span><span class="n">messages</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">prompt</span><span class="p">})</span>
        
        <span class="k">with</span> <span class="n">st</span><span class="p">.</span><span class="nf">chat_message</span><span class="p">(</span><span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">):</span>
            <span class="n">st</span><span class="p">.</span><span class="nf">markdown</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
            
        <span class="k">with</span> <span class="n">st</span><span class="p">.</span><span class="nf">chat_message</span><span class="p">(</span><span class="sh">"</span><span class="s">assistant</span><span class="sh">"</span><span class="p">):</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">chatbot</span><span class="p">.</span><span class="nf">get_response</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">st</span><span class="p">.</span><span class="n">session_state</span><span class="p">.</span><span class="n">chat_history</span><span class="p">)</span>
            <span class="n">st</span><span class="p">.</span><span class="nf">markdown</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
            
        <span class="n">st</span><span class="p">.</span><span class="n">session_state</span><span class="p">.</span><span class="n">messages</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">assistant</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">response</span><span class="p">})</span>
</code></pre></div></div>

<h2 id="advanced-features-and-optimizations">Advanced Features and Optimizations</h2>

<h3 id="context-window-management">Context Window Management</h3>

<p>One crucial aspect of RAG systems is managing the context window effectively. Here’s an implementation of a sliding window approach:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">manage_context_window</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">chat_history</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">3000</span><span class="p">):</span>
    <span class="n">total_tokens</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">managed_history</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">query</span><span class="p">,</span> <span class="n">response</span> <span class="ow">in</span> <span class="nf">reversed</span><span class="p">(</span><span class="n">chat_history</span><span class="p">):</span>
        <span class="n">estimated_tokens</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">query</span><span class="p">.</span><span class="nf">split</span><span class="p">())</span> <span class="o">+</span> <span class="nf">len</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="nf">split</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">total_tokens</span> <span class="o">+</span> <span class="n">estimated_tokens</span> <span class="o">&gt;</span> <span class="n">max_tokens</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="n">managed_history</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">query</span><span class="p">,</span> <span class="n">response</span><span class="p">))</span>
        <span class="n">total_tokens</span> <span class="o">+=</span> <span class="n">estimated_tokens</span>
    
    <span class="k">return</span> <span class="nf">list</span><span class="p">(</span><span class="nf">reversed</span><span class="p">(</span><span class="n">managed_history</span><span class="p">))</span>
</code></pre></div></div>

<h3 id="document-preprocessing-pipeline">Document Preprocessing Pipeline</h3>

<p>Implementing a robust document preprocessing pipeline is crucial for effective retrieval:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">preprocess_documents</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">documents</span><span class="p">):</span>
    <span class="c1"># Remove boilerplate content
</span>    <span class="n">cleaned_docs</span> <span class="o">=</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="nf">remove_boilerplate</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">]</span>
    
    <span class="c1"># Deduplicate similar content
</span>    <span class="n">unique_docs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">deduplicate_content</span><span class="p">(</span><span class="n">cleaned_docs</span><span class="p">)</span>
    
    <span class="c1"># Extract key information
</span>    <span class="n">processed_docs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">unique_docs</span><span class="p">:</span>
        <span class="n">metadata</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">extract_metadata</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
        <span class="n">processed_docs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nc">Document</span><span class="p">(</span>
            <span class="n">page_content</span><span class="o">=</span><span class="n">doc</span><span class="p">.</span><span class="n">page_content</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="o">**</span><span class="n">doc</span><span class="p">.</span><span class="n">metadata</span><span class="p">,</span> <span class="o">**</span><span class="n">metadata</span><span class="p">}</span>
        <span class="p">))</span>
    
    <span class="k">return</span> <span class="n">processed_docs</span>
</code></pre></div></div>

<h2 id="performance-optimization-and-scaling">Performance Optimization and Scaling</h2>

<h3 id="vector-store-optimization">Vector Store Optimization</h3>

<p>For production deployments, consider these optimizations for the vector store:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.vectorstores</span> <span class="kn">import</span> <span class="n">Chroma</span>
<span class="kn">from</span> <span class="n">langchain.embeddings</span> <span class="kn">import</span> <span class="n">HuggingFaceEmbeddings</span>

<span class="k">class</span> <span class="nc">OptimizedVectorStore</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embedding_function</span> <span class="o">=</span> <span class="nc">HuggingFaceEmbeddings</span><span class="p">(</span>
            <span class="n">model_name</span><span class="o">=</span><span class="sh">"</span><span class="s">all-MiniLM-L6-v2</span><span class="sh">"</span><span class="p">,</span>
            <span class="n">model_kwargs</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">device</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">}</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">vector_store</span> <span class="o">=</span> <span class="nc">Chroma</span><span class="p">(</span>
            <span class="n">persist_directory</span><span class="o">=</span><span class="sh">"</span><span class="s">./vector_store</span><span class="sh">"</span><span class="p">,</span>
            <span class="n">embedding_function</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">embedding_function</span><span class="p">,</span>
            <span class="n">collection_metadata</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">hnsw:space</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">cosine</span><span class="sh">"</span><span class="p">}</span>
        <span class="p">)</span>
</code></pre></div></div>

<h3 id="caching-and-response-generation">Caching and Response Generation</h3>

<p>Implement caching to improve response times:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@st.cache_data</span><span class="p">(</span><span class="n">ttl</span><span class="o">=</span><span class="mi">3600</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">get_cached_response</span><span class="p">(</span><span class="n">query_hash</span><span class="p">,</span> <span class="n">chat_history_hash</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">stored_responses</span><span class="p">.</span><span class="nf">get</span><span class="p">((</span><span class="n">query_hash</span><span class="p">,</span> <span class="n">chat_history_hash</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">generate_response</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">chat_history</span><span class="p">):</span>
    <span class="n">query_hash</span> <span class="o">=</span> <span class="nf">hash</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    <span class="n">chat_history_hash</span> <span class="o">=</span> <span class="nf">hash</span><span class="p">(</span><span class="nf">str</span><span class="p">(</span><span class="n">chat_history</span><span class="p">))</span>
    
    <span class="c1"># Check cache first
</span>    <span class="n">cached_response</span> <span class="o">=</span> <span class="nf">get_cached_response</span><span class="p">(</span><span class="n">query_hash</span><span class="p">,</span> <span class="n">chat_history_hash</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">cached_response</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">cached_response</span>
    
    <span class="c1"># Generate new response if not in cache
</span>    <span class="n">response</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">chain</span><span class="p">({</span><span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">:</span> <span class="n">query</span><span class="p">,</span> <span class="sh">"</span><span class="s">chat_history</span><span class="sh">"</span><span class="p">:</span> <span class="n">chat_history</span><span class="p">})</span>
    
    <span class="c1"># Store in cache
</span>    <span class="nf">store_response</span><span class="p">(</span><span class="n">query_hash</span><span class="p">,</span> <span class="n">chat_history_hash</span><span class="p">,</span> <span class="n">response</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">response</span>
</code></pre></div></div>

<h2 id="deployment-and-production-considerations">Deployment and Production Considerations</h2>

<h3 id="load-balancing-and-scaling">Load Balancing and Scaling</h3>

<p>For production deployments, implement load balancing and scaling strategies:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">concurrent.futures</span> <span class="kn">import</span> <span class="n">ThreadPoolExecutor</span>
<span class="kn">import</span> <span class="n">queue</span>

<span class="k">class</span> <span class="nc">LoadBalancedRAGChatbot</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">request_queue</span> <span class="o">=</span> <span class="n">queue</span><span class="p">.</span><span class="nc">Queue</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">workers</span> <span class="o">=</span> <span class="p">[</span><span class="nc">RAGChatbot</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_workers</span><span class="p">)]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">executor</span> <span class="o">=</span> <span class="nc">ThreadPoolExecutor</span><span class="p">(</span><span class="n">max_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">process_request</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">worker_id</span><span class="p">,</span> <span class="n">request</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">workers</span><span class="p">[</span><span class="n">worker_id</span><span class="p">].</span><span class="nf">generate_response</span><span class="p">(</span><span class="o">**</span><span class="n">request</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">handle_request</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">chat_history</span><span class="p">):</span>
        <span class="n">worker_id</span> <span class="o">=</span> <span class="nf">hash</span><span class="p">(</span><span class="n">query</span><span class="p">)</span> <span class="o">%</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">workers</span><span class="p">)</span>
        <span class="n">request</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">query</span><span class="sh">"</span><span class="p">:</span> <span class="n">query</span><span class="p">,</span> <span class="sh">"</span><span class="s">chat_history</span><span class="sh">"</span><span class="p">:</span> <span class="n">chat_history</span><span class="p">}</span>
        
        <span class="n">future</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">executor</span><span class="p">.</span><span class="nf">submit</span><span class="p">(</span>
            <span class="n">self</span><span class="p">.</span><span class="n">process_request</span><span class="p">,</span>
            <span class="n">worker_id</span><span class="p">,</span>
            <span class="n">request</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">future</span><span class="p">.</span><span class="nf">result</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="conclusion">Conclusion</h2>

<p>Building a RAG-powered chatbot with Streamlit and LangChain represents a powerful approach to creating intelligent conversational systems. The combination of Streamlit’s user-friendly interface, LangChain’s robust orchestration capabilities, and RAG’s dynamic knowledge integration provides a solid foundation for building sophisticated AI applications.</p>

<p>As the field continues to evolve, we can expect to see further improvements in areas such as:</p>
<ul>
  <li>More efficient retrieval algorithms</li>
  <li>Better context window management</li>
  <li>Enhanced response generation techniques</li>
  <li>Improved vector store optimizations</li>
</ul>

<p>The key to success lies in carefully balancing these components while maintaining focus on the end-user experience and system performance.</p>

<h2 id="references">References</h2>

<p><a href="https://python.langchain.com/docs/get_started/introduction">LangChain Documentation</a>
<a href="https://docs.streamlit.io">Streamlit Documentation</a>
<a href="https://arxiv.org/abs/2005.11401">RAG: Neural Information Retrieval Enhanced with Generation</a>
<a href="https://docs.trychroma.com/">Chroma Vector Store Documentation</a></p>


        </div>
        <div class="footer">
    
        <br>
    
</div>

    </div>
</body>
